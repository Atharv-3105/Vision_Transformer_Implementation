# Vision Transformer (ViT) - From Scratch Implementation 🧠📊

## 📄 Overview

This project contains a **from-scratch implementation of the Vision Transformer (ViT)** architecture based on the research paper:

> **"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**  
> *Alexey Dosovitskiy et al., 2020*  

All key components of the Vision Transformer have been manually implemented using PyTorch, without relying on pre-built ViT libraries like huggingface.

---

## ✅ Features

- 🔧 **Complete Custom ViT Implementation**
  - Patch Embedding
  - Positional Encoding
  - Transformer Encoder Blocks
  - Classification Head
  - All built from scratch using PyTorch.

- 📦 **Custom DataLoader Classes**
  - Independent DataLoader classes implemented from scratch for:
    - **MNIST** (Grayscale handwritten digits dataset)
    - **CIFAR-10** (RGB image dataset of 10 object categories)

- 📊 **Training and Evaluation**
  - Fully trained and evaluated the ViT model on:
    - **MNIST**
    - **CIFAR-10**

---
- 🧩 Vision Transformer Components Implemented
  - ✅ Patch Embedding

  - ✅ Learnable Positional Encoding

  - ✅ Multi-Head Self Attention

  - ✅ Feed-Forward Network (MLP Head)

  - ✅ Transformer Encoder Blocks

  - ✅ Layer Normalization

  - ✅ Classification Head
---
## ⚠️Note:- This implementation focuses on architecture reproduction and conceptual clarity, rather than achieving state-of-the-art results.

---
- ## 📚 References

  - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Original Paper)](https://arxiv.org/abs/2010.11929)
  


