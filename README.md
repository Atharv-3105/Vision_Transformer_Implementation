# Vision Transformer (ViT) - From Scratch Implementation ğŸ§ ğŸ“Š

## ğŸ“„ Overview

This project contains a **from-scratch implementation of the Vision Transformer (ViT)** architecture based on the research paper:

> **"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**  
> *Alexey Dosovitskiy et al., 2020*  

All key components of the Vision Transformer have been manually implemented using PyTorch, without relying on pre-built ViT libraries like huggingface.

---

## âœ… Features

- ğŸ”§ **Complete Custom ViT Implementation**
  - Patch Embedding
  - Positional Encoding
  - Transformer Encoder Blocks
  - Classification Head
  - All built from scratch using PyTorch.

- ğŸ“¦ **Custom DataLoader Classes**
  - Independent DataLoader classes implemented from scratch for:
    - **MNIST** (Grayscale handwritten digits dataset)
    - **CIFAR-10** (RGB image dataset of 10 object categories)

- ğŸ“Š **Training and Evaluation**
  - Fully trained and evaluated the ViT model on:
    - **MNIST**
    - **CIFAR-10**

---
- ğŸ§© Vision Transformer Components Implemented
  - âœ… Patch Embedding

  - âœ… Learnable Positional Encoding

  - âœ… Multi-Head Self Attention

  - âœ… Feed-Forward Network (MLP Head)

  - âœ… Transformer Encoder Blocks

  - âœ… Layer Normalization

  - âœ… Classification Head
---
## âš ï¸Note:- This implementation focuses on architecture reproduction and conceptual clarity, rather than achieving state-of-the-art results.

---
- ## ğŸ“š References

  - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Original Paper)](https://arxiv.org/abs/2010.11929)
  


